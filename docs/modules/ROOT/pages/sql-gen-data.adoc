////
Make sure to rename this file to the name of your repository and add the filename to the README. This filename must not conflict with any existing tutorials.
////

// Describe the title of your article by replacing 'Tutorial template' with the page name you want to publish.
= Generating Streaming data using SQL
// Add required variables
:page-layout: tutorial
:page-product: cloud // Required: Define the product filter for this tutorial. Add one of the following: platform, imdg, cloud, operator
:page-categories: Stream Processing, Get Started, SQL  // Optional: Define the categories for this tutorial. Check the current categories on the tutorials homepage (https://docs.hazelcast.com/tutorials/). Add one or more of the existing categories or add new ones as a comma-separated list. Make sure that you use title case for all categories. 
:page-lang: sql // Optional: Define what Hazelcast client languages are supported by this tutorial. Leave blank or add one or more of: java, go, python, cplus, node, csharp.
:page-enterprise: // Required: Define whether this tutorial requires an Enterprise license (true or blank)
:page-est-time: 10 mins // Required: Define the estimated number of time required to complete the tutorial in minutes. For example, 10 mins
:description: Use SQL on Hazelcast to generate randomized streaming data for demo/POC purposes. 
// Required: Summarize what this tutorial is about in a sentence or two. What you put here is reused as the tutorial's first paragraph and included in HTML description tags. Start the sentence with an action verb such as 'Deploy' or 'Connect'.

{description}

// Give some context about the use case for this tutorial. What will the reader learn?
== Context
In this tutorial, you will learn how to use SQL to generate streaming data locally to Hazelcast.

Using the `VIEW` and `generate-stream` functions of SQL on Hazelcast, you can create a data stream locally within Hazelcast. 

As of Hazelcast 5.3, you can:

* Access this data directly via SQL functionality in any programming language.

* Direct the output to an external Kafka server, then use the connection to the Kafka server to access the data via the pipeline API or via SQL using `CREATE MAPPING`.

[NOTE]
====
In an upcoming release of Hazelcast, the Kafka Connect connector will expose SQL, eliminating the need for an external Kafka server to feed data into the pipeline API. 
====

// Optional: What does the reader need before starting this tutorial? Think about tools or knowledge. Delete this section if your readers can dive straight into the lesson without requiring any prerequisite knowledge.
== Before you Begin

Before starting this tutorial, make sure that you meet the following prerequisites:

* Running cluster of Hazelcast
* Connection to SQL command line, either through CLC or through Management Center
* (Optional) a Kafka instance accessible by your Hazelcast cluster


== Step 1. Generating Data

////
Introduce what your audience will learn in each step, then continue to write the steps in the tutorial.
You can choose one of these approaches to write your tutorial part:

* In a narrative style if your parts are short or you are using screenshots to do most of the talking.   
* In a "Goal > Steps > Outcome" structure to build a predictable flow in all your tutorial parts.

Whatever option you choose when designing your tutorial should be carried through in subsequent parts.
////

The following code is from the link:https://docs.hazelcast.com/tutorials/SQL-Basics-on-Viridian[SQL Basics on Viridian (Stock Ticker Demo) tutorial]. The comments break down what each part of the code is doing.

```sql
CREATE OR REPLACE VIEW trades AS
  SELECT id,

       CASE WHEN tickRand BETWEEN 0 AND 0.1 THEN 'APPL'
            WHEN tickRand BETWEEN 0.1 AND 0.2 THEN 'GOOGL'
            WHEN tickRand BETWEEN 0.2 AND 0.3 THEN 'META'
            WHEN tickRand BETWEEN 0.3 AND 0.4 THEN 'NFLX'
            WHEN tickRand BETWEEN 0.4 AND 0.5 THEN 'AMZN'
            WHEN tickRand BETWEEN 0.5 AND 0.6 THEN 'INTC'
            WHEN tickRand BETWEEN 0.6 AND 0.7 THEN 'CSCO'
            WHEN tickRand BETWEEN 0.7 AND 0.8 THEN 'BABA'
            ELSE 'VOO'
       END as ticker, <1>

       CASE WHEN tickRand BETWEEN 0 and 0.1 then tickRand*50+1
            WHEN tickRand BETWEEN 0.1 AND 0.2 THEN tickRand*75+.6
            WHEN tickRand BETWEEN 0.2 AND 0.3 THEN tickRand*60+.2
            WHEN tickRand BETWEEN 0.3 AND 0.4 THEN tickRand*30+.3
            WHEN tickRand BETWEEN 0.4 AND 0.5 THEN tickRand*43+.7
            WHEN tickRand BETWEEN 0.5 AND 0.6 THEN tickRand*100+.4
            WHEN tickRand BETWEEN 0.6 AND 0.7 THEN tickRand*25+.8
            WHEN tickRand BETWEEN 0.6 AND 0.7 THEN tickRand*80+.5
            WHEN tickRand BETWEEN 0.7 AND 0.8 THEN tickRand*10+.1
            ELSE tickRand*100+4
       END as price,<2>

       trade_ts,
       amount
FROM
    (SELECT v as id,
           RAND(v*v) as tickRand,<3>
           TO_TIMESTAMP_TZ(v*10 + 1645484400000) as trade_ts, <4>
           ROUND(RAND()*100, 0) as amount
     FROM TABLE(generate_stream(100))); <5>
```
<1> We're using the random number to generate different stock ticker symbols.
<2> To keep each ticker's price within a reasonable range of variation, we use the same `BETWEEN` ranges, and give each one a different base multiplier. 
<3> The random number generator creates the `tickRand` value.
<4> We seed the timestamp with a base value that equates to 21 Feb 2022. You can change this to any reasonable Unix timestamp. 
<5> The `generate_stream` function is what makes this all work. In this example, we're generating 100 events per second. 

Once this view is created, you can access it via SQL. Because you're looking at streaming data, you'll need to use CTRL-C to stop each query.

```sql
SELECT * from trades;

SELECT ticker AS Symbol, ROUND(price,2) AS Price, amount AS "Shares Sold"
FROM trades;
```



== Step 2. Inserting into Kafka

////
Continue the design approach you chose in the previous part and continue it through to the end of the tutorial.
////

You can send generated data to Kafka. Kafka will store and replay it as it would data from any other streaming source. Instead of creating a view local to Hazelcast, you'll create a mapping within SQL for the Kafka topic, then use the `INSERT` function to send generated data to that topic.

. First, create a mapping for the data. Include all the fields that you'll generate with SQL. This creates a topic in Kafka as well as m
+
```sql
CREATE or REPLACE MAPPING trades (
    id BIGINT,
    ticker VARCHAR,
    price DECIMAL,
    trade_ts TIMESTAMP WITH TIME ZONE,
    amount BIGINT)
TYPE Kafka
OPTIONS (
    'valueFormat' = 'json-flat',
    'bootstrap.servers' = 'broker:9092'
);
```
. Next, use the `INSERT` function to send the data to the `trades` topic you just created.
```sql
INSERT INTO trades
  SELECT id,
       CASE WHEN tickRand BETWEEN 0 AND 0.1 THEN 'APPL'
            WHEN tickRand BETWEEN 0.1 AND 0.2 THEN 'GOOGL'
            WHEN tickRand BETWEEN 0.2 AND 0.3 THEN 'META'
            WHEN tickRand BETWEEN 0.3 AND 0.4 THEN 'NFLX'
            WHEN tickRand BETWEEN 0.4 AND 0.5 THEN 'AMZN'
            WHEN tickRand BETWEEN 0.5 AND 0.6 THEN 'INTC'
            WHEN tickRand BETWEEN 0.6 AND 0.7 THEN 'CSCO'
            WHEN tickRand BETWEEN 0.7 AND 0.8 THEN 'BABA'
            ELSE 'VOO'
       END as ticker,
       CASE WHEN tickRand BETWEEN 0 and 0.1 then tickRand*50+1
            WHEN tickRand BETWEEN 0.1 AND 0.2 THEN tickRand*75+.6
            WHEN tickRand BETWEEN 0.2 AND 0.3 THEN tickRand*60+.2
            WHEN tickRand BETWEEN 0.3 AND 0.4 THEN tickRand*30+.3
            WHEN tickRand BETWEEN 0.4 AND 0.5 THEN tickRand*43+.7
            WHEN tickRand BETWEEN 0.5 AND 0.6 THEN tickRand*100+.4
            WHEN tickRand BETWEEN 0.6 AND 0.7 THEN tickRand*25+.8
            WHEN tickRand BETWEEN 0.6 AND 0.7 THEN tickRand*80+.5
            WHEN tickRand BETWEEN 0.7 AND 0.8 THEN tickRand*10+.1
            ELSE tickRand*100+4
       END as price,
       trade_ts,
       amount
FROM
    (SELECT v as id,
           RAND(v*v) as tickRand,<3>
           TO_TIMESTAMP_TZ(v*10 + 1645484400000) as trade_ts, <4>
           ROUND(RAND()*100, 0) as amount
     FROM TABLE(generate_stream(100))); <5>
```
The code to generate the data is exactly the same; the only difference is that we're sending it to Kafka instead of creating a local view.

== Summary

////
Summarise what knowledge the reader has gained by completing the tutorial, including a summary of each step's goals (this is a good way to validate whether your tutorial has covered all you need it to.)
////
You can now use SQL on Hazelcast to generate streaming data for testing/demo purposes. 

